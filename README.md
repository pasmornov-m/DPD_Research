## Описание

Проект реализует сравнительный анализ различных архитектур обучения цифровых корректоров (DPD) для компенсации нелинейных искажений в СВЧ усилителях мощности на основе обобщённой полиномиальной модели с памятью (GMP). В работе рассматриваются три подхода:

* **Прямая архитектура обучения (DLA)**
* **Обратная архитектура обучения (ILA)**
* **Итеративный контроль обучения (ILC)**

Реализованы функции загрузки и подготовки данных, обучение моделей, применение предыскажения, вычисление метрик (NMSE, ACPR), а также визуализация результатов (спектры, AM/AM и AM/PM характеристики, зависимости метрик от отношения сигнал/шум).

Основная цель — провести симуляционное моделирование и сравнить эффективность DLA, ILA и ILC при различных условиях (уровень шума, параметры модели).


## Основные возможности

* Загрузка и подготовка тренировочных и валидационных данных сигнала (например, OFDM сигнал с заданной полосой и частотой дискретизации).
* Построение и оптимизация обобщённой полиномиальной модели с памятью (GMP) для моделирования усилителя мощности и корректора.
* Реализация алгоритмов обучения корректоров:

  * Прямая архитектура обучения (DLA) через минимизацию ошибки между желаемым выходом и реальным ответом усилителя.
  * Обратная архитектура обучения (ILA) через построение обратной модели усилителя по выходному сигналу.
  * Итеративный контроль обучения (ILC) через итеративную оптимизацию входного сигнала до достижения нужной точности, с последующей идентификацией модели корректра.
* Вычисление ключевых метрик:

  * **NMSE (Normalized Mean Squared Error)** между выходом модели и целевым линейным выходом.
  * **ACPR (Adjacent Channel Power Ratio)** для оценки внеполосного излучения.
* Влияние шума: исследование зависимости эффективности алгоритмов от отношения сигнал/шум (SNR).
* Визуализация результатов:

  * Спектральная плотность мощности входного, выходного и скорректированного сигналов.
  * AM/AM и AM/PM характеристики усилителя и корректируемых сигналов.
  * Графики зависимости NMSE и ACPR от SNR для разных методов.
* Сохранение и загрузка коэффициентов моделей, чтобы избегать повторных долгих оптимизаций.
* Модульная структура кода для лёгкого расширения (добавление новых моделей или архитектур обучения).

---

### Обобщённая полиномиальная модель (GMP)

$$
y_{\text{GMP}}(n) =
\sum_{k=1}^{K_a} \sum_{l=0}^{L_a} a_{kl} \, x(n - l) \, |x(n - l)|^k +
\sum_{k=1}^{K_b} \sum_{l=0}^{L_b} \sum_{m=1}^{M_b} b_{klm} \, x(n - l) \, |x(n - l - m)|^k +
\sum_{k=1}^{K_c} \sum_{l=0}^{L_c} \sum_{m=1}^{M_c} c_{klm} \, x(n - l) \, |x(n - l + m)|^k
$$

### Обозначения:

* $x(n)$ — входной сигнал;
* $y_{\text{GMP}}(n)$ — выходной сигнал модели;
* $a_{kl}, b_{klm}, c_{klm}$ — коэффициенты модели;
* $K_a, K_b, K_c$ — максимальная степень нелинейности (нечетность обычно: 1, 3, 5, …);
* $L_a, L_b, L_c$ — глубина линейной памяти (задержка $l$);
* $M_b, M_c$ — глубина перекрёстной памяти (задержка $m$);
* Первая сумма — **основной нелинейный полином с памятью**;
* Вторая — **отстающая перекрёстная память**;
* Третья — **опережающая перекрёстная память**.

